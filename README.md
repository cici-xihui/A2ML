# A2ML


Due to their ever-increasing number of current applications, multi-label classification algorithms are facing a major challenge: their capacity to learn models from stream data that include changes in distribution over time, while constantly coming running up against limited computational and storage resources. In this paper, we first revisit some existing works as a meta-model of multi-scale algorithms of low-complexity based on a combination of two memories: a short-term memory for fast drift adaptation and a long-term one to integrate the distribution changes over time. Then, we develop a new algorithm of this family, called A2ML (Adaptive Memories for Multi-Label stream classification), specifically designed for non-stationary streams. The management of its long-term memory based on an adaptive clustering of the label vector set associated to a biased reservoir sampling strategy for the information storage guarantees the linearly limited complexity of updating the learning model. A2ML is compared to $7$ state-of-the-art algorithms using $15$ stationary data streams and $4$ non-stationary ones of over $100,000$ examples, which are generated via a tailor-made protocol designed to specifically assess different modalities of data distribution changes in the stream, as well as various types of concept drift. The results confirm A2MLâ€™s high levels of performance in both contexts, and reveal computation times that are lower than those of its rivals.
